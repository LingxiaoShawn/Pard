{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_noise: tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]), torch.Size([6])\n",
      "edge_noise: tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]), torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "from pard.parallel.utils import find_checkpoint_with_lowest_val_loss\n",
    "from pard.parallel.task import AutoregressiveDiffusion, PredictBlockProperties\n",
    "\n",
    "device = 0\n",
    "\n",
    "blocksize_model_dir = 'checkpoints/block_prediction/qm9.3hops.ppgnTrans-Parallel.BlockID11.bn.PreNorm=1.H256.E64.L8-lr0.0004.cosine'\n",
    "diffusion_model_dir = 'checkpoints/local_denoising/qm9.3hops.ppgnTrans-Parallel.BlockID11.bn.PreNorm=1.H256.E64.L8-lr0.0004.cosine-ires1.blocktime0.uni_noise1.T20.cosine.vlb1.ce0.1.combine=False'\n",
    "blocksize_path = find_checkpoint_with_lowest_val_loss(blocksize_model_dir)\n",
    "diffusion_path = find_checkpoint_with_lowest_val_loss(diffusion_model_dir)\n",
    "\n",
    "blocksize_model = PredictBlockProperties.load_from_checkpoint(blocksize_path, map_location=f'cuda:{device}')\n",
    "diffusion_model = AutoregressiveDiffusion.load_from_checkpoint(diffusion_path, combine_training=False, map_location=f'cuda:{device}')\n",
    "diffusion_model.blocksize_model = blocksize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pard.utils import check_block_id_train_vs_generation\n",
    "generated_batch = diffusion_model.generate(100).to('cpu') \n",
    "generated_list = check_block_id_train_vs_generation(generated_batch.nodes, generated_batch.edges, generated_batch.nodes_blockid, train_max_hops=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 95.0\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "l.extend(generated_list)\n",
    "m = 100*sum(l) / len(l)\n",
    "print('test',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of graphs that have the same generation block path as training block path: 95.0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "print('Percentage of graphs that have the same generation block path as training block path:',\n",
    "                100*sum(l) / len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=85-val_loss=2.042.ckpt 2.042\n",
      "epoch=99-val_loss=2.049.ckpt 2.049\n",
      "epoch=94-val_loss=2.048.ckpt 2.048\n",
      "epoch=83-val_loss=2.047.ckpt 2.047\n",
      "epoch=91-val_loss=2.046.ckpt 2.046\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "def find_checkpoint_with_lowest_val_loss(directory):\n",
    "    lowest_val_loss = float('inf')\n",
    "    best_checkpoint = None\n",
    "    # Regex to match the pattern in the filename and extract the loss value\n",
    "    pattern = re.compile(r'epoch=([\\d.]+)+-val_loss=([\\d.]+)\\.ckpt')\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        match = pattern.search(filename)\n",
    "        # print(match.group(0))\n",
    "        if match:\n",
    "            print(match.group(0), match.group(1))\n",
    "            val_loss = float(match.group(1))\n",
    "            # epoch = float(match.group(0))\n",
    "find_checkpoint_with_lowest_val_loss(diffusion_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LocalDiffusion.dataset import DATA_INFO\n",
    "dataset = 'caveman'\n",
    "data_info_dict = DATA_INFO[dataset]\n",
    "atom_decoder = data_info_dict.get('atom_decoder', None)\n",
    "metric_class = data_info_dict.get('metric_class', None)\n",
    "original_datasets = {split:data_info_dict['class'](**(data_info_dict['default_args'] | {'split':split})) for split in ['train', 'val', 'test']}\n",
    "\n",
    "print( sum(len(dataset) for dataset in original_datasets.values()))\n",
    "print('num_nodes avg:', sum([x.num_nodes for x in original_datasets['train']]) / len(original_datasets['train']))\n",
    "print('num_edges avg:', sum([x.num_edges for x in original_datasets['train']]) / len(original_datasets['train']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
